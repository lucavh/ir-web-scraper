{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter,defaultdict\n",
    "import pickle\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Creating inverted indexes for term frequency and term positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [re.sub(r'[^\\w\\s]','',w) for w in nltk.word_tokenize(text.lower()) if re.sub(r'[^\\w\\s]','',w) != '']\n",
    "   \n",
    "def remove_stopwords(tokens):\n",
    "    en_stopwords = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in en_stopwords]\n",
    "\n",
    "def stemmer(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU ONLY HAVE TO CHANGE THIS BLOCK OF CODE\n",
    "\n",
    "# Filename\n",
    "source = 'data-mw.json' # CHANGE THIS\n",
    "\n",
    "# Open json file \n",
    "with open(source) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Make dataframe from data file \n",
    "df = pd.DataFrame(data) # MAKE SURE YOUR DATAFRAME HAS A COLUMN CALLED 'url'\n",
    "\n",
    "# Create small subset for debugging (can be commented out for final run)\n",
    "#df = df.head(100)\n",
    "\n",
    "# Tokenize content, remove stopwords and stem\n",
    "# Note : the following line includes the 'title' and 'content' column\n",
    "df['tokenized'] = df.apply(lambda row: stemmer(remove_stopwords(tokenize(row.title + ' ' + row.content))), axis=1) # CHANGE THIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverted index\n",
    "invertedIndexFreq = defaultdict(Counter)\n",
    "invertedIndexPos = defaultdict(dict)\n",
    "corpusInfo = defaultdict(dict)\n",
    "corpusInfo['num_docs'] = df.shape[0]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    corpusInfo['doc_lengths'][row['url']] = len(row['tokenized'])\n",
    "        \n",
    "    for w in row['tokenized']:\n",
    "        invertedIndexFreq[w][row['url']]+=1\n",
    "        invertedIndexPos[w][row['url']] = [i for i, j in enumerate(row['tokenized']) if w == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the inverted index with frequences\n",
    "with open(source[:-5]+'-invertedIndexFreq.pickle', 'wb') as handle:\n",
    "    pickle.dump(invertedIndexFreq, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the inverted index with positions\n",
    "with open(source[:-5]+'-invertedIndexPos.pickle', 'wb') as handle:\n",
    "    pickle.dump(invertedIndexPos, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the corpus info\n",
    "with open(source[:-5]+'-corpusInfo.pickle', 'wb') as handle:\n",
    "    pickle.dump(corpusInfo, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Combining all indexes into one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FREQUENCY INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inverted indexes\n",
    "ii_freq_imdb = pickle.load( open( \"data-imdb-invertedIndexFreq.pickle\", \"rb\" ) )\n",
    "ii_freq_tmz = pickle.load( open( \"data-tmz-invertedIndexFreq.pickle\", \"rb\" ) )\n",
    "ii_freq_mw = pickle.load( open( \"data-mw-invertedIndexFreq.pickle\", \"rb\" ) )\n",
    "ii_freq_rr = pickle.load( open( \"data-rr-invertedIndexFreq.pickle\", \"rb\" ) )\n",
    "ii_freq_hl = pickle.load( open( \"data-hl-invertedIndexFreq.pickle\", \"rb\" ) )\n",
    "ii_freq_rt = pickle.load( open( \"data-rt-invertedIndexFreq.pickle\", \"rb\" ) )\n",
    "\n",
    "# Initialize final frequency index dict \n",
    "ii_freq_merged = defaultdict(Counter)\n",
    "\n",
    "# Merge indexes\n",
    "for k,v in chain(ii_freq_imdb.items(), ii_freq_tmz.items(), \n",
    "                 ii_freq_mw.items(), ii_freq_rr.items(),\n",
    "                 ii_freq_hl.items(), ii_freq_rt.items()):\n",
    "    \n",
    "    ii_freq_merged[k].update(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the corpus info\n",
    "with open('merged-invertedIndexFreq', 'wb') as handle:\n",
    "    pickle.dump(ii_freq_merged, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POSITION INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inverted indexes\n",
    "ii_pos_imdb = pickle.load( open( \"data-imdb-invertedIndexPos.pickle\", \"rb\" ) )\n",
    "ii_pos_tmz = pickle.load( open( \"data-tmz-invertedIndexPos.pickle\", \"rb\" ) )\n",
    "ii_pos_mw = pickle.load( open( \"data-mw-invertedIndexPos.pickle\", \"rb\" ) )\n",
    "ii_pos_rr = pickle.load( open( \"data-rr-invertedIndexPos.pickle\", \"rb\" ) )\n",
    "ii_pos_hl = pickle.load( open( \"data-hl-invertedIndexPos.pickle\", \"rb\" ) )\n",
    "ii_pos_rt = pickle.load( open( \"data-rt-invertedIndexPos.pickle\", \"rb\" ) )\n",
    "\n",
    "# Initialize final position index dict \n",
    "ii_pos_merged = ii_pos_imdb\n",
    "\n",
    "# Merge indexes\n",
    "for w, dic in ii_pos_tmz.items():\n",
    "    for doc, lis in dic.items():\n",
    "        ii_pos_merged[w][doc] = lis\n",
    "        \n",
    "for w, dic in ii_pos_mw.items():\n",
    "    for doc, lis in dic.items():\n",
    "        ii_pos_merged[w][doc] = lis\n",
    "        \n",
    "for w, dic in ii_pos_rr.items():\n",
    "    for doc, lis in dic.items():\n",
    "        ii_pos_merged[w][doc] = lis\n",
    "        \n",
    "for w, dic in ii_pos_hl.items():\n",
    "    for doc, lis in dic.items():\n",
    "        ii_pos_merged[w][doc] = lis\n",
    "        \n",
    "for w, dic in ii_pos_rt.items():\n",
    "    for doc, lis in dic.items():\n",
    "        ii_pos_merged[w][doc] = lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the corpus info\n",
    "with open('merged-invertedIndexPos.pickle', 'wb') as handle:\n",
    "    pickle.dump(ii_pos_merged, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORPUS INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus infos\n",
    "ci_imdb = pickle.load( open( \"data-imdb-corpusInfo.pickle\", \"rb\" ) )\n",
    "ci_tmz = pickle.load( open( \"data-tmz-corpusInfo.pickle\", \"rb\" ) )\n",
    "ci_mw = pickle.load( open( \"data-mw-corpusInfo.pickle\", \"rb\" ) )\n",
    "ci_rr = pickle.load( open( \"data-rr-corpusInfo.pickle\", \"rb\" ) )\n",
    "ci_hl = pickle.load( open( \"data-hl-corpusInfo.pickle\", \"rb\" ) )\n",
    "ci_rt = pickle.load( open( \"data-rt-corpusInfo.pickle\", \"rb\" ) )\n",
    "\n",
    "# Initialize final courpusinfo dict\n",
    "ci_merged = ci_imdb\n",
    "\n",
    "# Merge indexes\n",
    "ci_merged['num_docs'] += ci_tmz['num_docs'] \n",
    "for doc, length in ci_tmz['doc_lengths'].items():\n",
    "    ci_merged['doc_lengths'][doc] = length\n",
    "      \n",
    "ci_merged['num_docs'] += ci_mw['num_docs'] \n",
    "for doc, length in ci_mw['doc_lengths'].items():\n",
    "    ci_merged['doc_lengths'][doc] = length\n",
    "    \n",
    "ci_merged['num_docs'] += ci_rr['num_docs'] \n",
    "for doc, length in ci_rr['doc_lengths'].items():\n",
    "    ci_merged['doc_lengths'][doc] = length\n",
    "        \n",
    "ci_merged['num_docs'] += ci_hl['num_docs'] \n",
    "for doc, length in ci_hl['doc_lengths'].items():\n",
    "    ci_merged['doc_lengths'][doc] = length\n",
    "    \n",
    "ci_merged['num_docs'] += ci_rt['num_docs'] \n",
    "for doc, length in ci_rt['doc_lengths'].items():\n",
    "    ci_merged['doc_lengths'][doc] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the corpus info\n",
    "with open('merged-corpusInfo.pickle', 'wb') as handle:\n",
    "    pickle.dump(ci_merged, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Computing TF-IDF\n",
    "We can only compute the tf-idf if we have the word frequency in the WHOLE corpus (ALL documents), thus when all sources are combined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tf-idf\n",
    "def tfidf(doc_freq, doc_length, n_docs_total, n_docs_containing):\n",
    "    return (doc_freq / doc_length) * math.log(n_docs_total / (1 + n_docs_containing))\n",
    "\n",
    "# Load merged files\n",
    "ii_freq_merged = pickle.load( open( \"merged-invertedIndexFreq.pickle\", \"rb\" ) )\n",
    "ci_merged = pickle.load( open( \"merged-corpusInfo.pickle\", \"rb\" ) )\n",
    "\n",
    "# Initialize final weights index dict \n",
    "ii_w_merged = defaultdict(Counter)\n",
    "\n",
    "for w,wv in ii_freq_merged.items():\n",
    "    n_docs_total = ci_merged['num_docs']\n",
    "    n_docs_containing = len(wv)\n",
    "    \n",
    "    for d_id,dv in wv.items():\n",
    "        doc_freq = dv\n",
    "        doc_length = ci_merged['doc_lengths'][d_id]\n",
    "        \n",
    "        ii_w_merged[w][d_id] = tfidf(doc_freq, doc_length, n_docs_total, n_docs_containing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the corpus info\n",
    "with open('merged-invertedIndexWeights.pickle', 'wb') as handle:\n",
    "    pickle.dump(ii_w_merged, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
