{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter,defaultdict\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# DEFS ----------------------------------------------------------------------------------------------------\n",
    "def load_data(file):\n",
    "    \n",
    "    with open(file) as f:\n",
    "        json_block = []\n",
    "        for line in f:\n",
    "            json_block.append(json.loads(line))\n",
    "            \n",
    "    return pd.DataFrame(json_block)\n",
    "\n",
    "def tokenize(text):\n",
    "    return [re.sub(r'[^\\w\\s]','',w) for w in nltk.word_tokenize(text.lower()) if re.sub(r'[^\\w\\s]','',w) != '']\n",
    "   \n",
    "def remove_stopwords(tokens):\n",
    "    en_stopwords = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in en_stopwords]\n",
    "\n",
    "def stemmer(tokens):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def find_relative_news(query):\n",
    "    relative = []\n",
    "    for index, row in news.iterrows():\n",
    "        if query.lower() in row.title.lower():\n",
    "            relative.append(row.url)\n",
    "        elif query.lower() in row.content.lower():\n",
    "            relative.append(row.url)\n",
    "    return relative\n",
    "\n",
    "def process_filmography(filmography_object):\n",
    "    filmography = []\n",
    "    for item in filmography_object:\n",
    "        movie_instance = {}\n",
    "        soup = BeautifulSoup(item, 'html.parser')\n",
    "        movie_instance['url'] = 'https://www.imdb.com' + soup.b.a.get('href')\n",
    "        movie_instance['data'] = ' '.join(soup.get_text().strip().replace('\\n',' ').split(' '))\n",
    "        filmography.append(movie_instance)\n",
    "    \n",
    "    return filmography\n",
    "\n",
    "def create_inv_indexes(df):\n",
    "    invertedIndexFreq = defaultdict(Counter)\n",
    "    invertedIndexPos = defaultdict(dict)\n",
    "    corpusInfo = defaultdict(dict)\n",
    "    \n",
    "    corpusInfo['num_docs'] = df.shape[0]\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        corpusInfo['doc_lengths'][row['url']] = len(row['tokenized'])\n",
    "        \n",
    "        for w in row['tokenized']:\n",
    "            invertedIndexFreq[w][row['url']]+=1\n",
    "            invertedIndexPos[w][row['url']] = [i for i, j in enumerate(row['tokenized']) if w == j]\n",
    "            \n",
    "    return invertedIndexFreq, invertedIndexPos, corpusInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ----------------------------------------------------------------------------------------------------\n",
    "actors_jl = '../data/actors.jl'\n",
    "movies_jl = '../data/movies.jl'\n",
    "tmz_jl = '../data/news_tmz.jl'\n",
    "hollywoodlife_jl = '../data/news_hollywoodlife.jl'\n",
    "movieweb_jl = '../data/news_movieweb.jl'\n",
    "\n",
    "#actors = load_data(actors_jl)\n",
    "#movies = load_data(movies_jl)\n",
    "tmz = load_data(tmz_jl)\n",
    "hollywoodlife = load_data(hollywoodlife_jl)\n",
    "movieweb = load_data(movieweb_jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS -----------------------------------------------------------------------------------------------\n",
    "#actors['filmography'] = actors.apply(lambda row: process_filmography(row.filmography), axis=1)\n",
    "#actors['movie_urls'] = actors.apply(lambda row: ['https://www.imdb.com' + url for url in row.movie_urls], axis=1)\n",
    "\n",
    "#movies['tokenized'] = movies.apply(lambda row: stemmer(remove_stopwords(tokenize(row.title + ' ' + row.year + ' ' + ' '.join(row.genres) + ' ' + ' '.join(row.reviews)))), axis=1)\n",
    "\n",
    "tmz['tokenized'] = tmz.apply(lambda row: stemmer(remove_stopwords(tokenize(row.title + ' ' + row.content))), axis=1)\n",
    "hollywoodlife['tokenized'] = hollywoodlife.apply(lambda row: stemmer(remove_stopwords(tokenize(row.title + ' ' + row.content))), axis=1)\n",
    "movieweb['tokenized'] = movieweb.apply(lambda row: stemmer(remove_stopwords(tokenize(row.title + ' ' + row.content))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATCH ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# TODO: MATCH ACTOR TO NEWS (BOTH ON ACTOR NAME AS WELL AS ON FILMOGRAPHY)\n",
    "# TODO: MATCH ACTOR TO MOVIE CONTENT\n",
    "\n",
    "# TODO: TOKENIZE ALL ACTOR CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEX ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# TODO: CREATE INVERTED INDEX - FREQ ACTOR\n",
    "# TODO: CREATE INVERTED INDEX - POS ACTOR\n",
    "# TODO: CREATE INVERTED INDEX - WEIGHT ACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT ------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# TODO: SAVE INV INDEX - FREQ AS PICKLE\n",
    "# TODO: SAVE INV INDEX - POS AS PICKLE\n",
    "# TODO: SAVE INV INDEX - WEIGHT AS PICKLE\n",
    "# TODO: SAVE ACTOR DF AS JSON\n",
    "# TODO: SAVE MOVIE DF AS JSON\n",
    "# TODO: SAVE NEWS DF AS JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_news = [item for l in news[news.url.isin(find_relative_news('naomi scott'))].tokenized.values for item in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors['movie_urls'] = actors.apply(lambda row: ['https://www.imdb.com' + url for url in row.movie_urls], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_actor_item(name, bio):\n",
    "    tokenized = []\n",
    "    \n",
    "    rel_news = news[news.url.isin(find_relative_news(name))]\n",
    "    tokenized += [item for l in rel_news.tokenized.values for item in l]\n",
    "    \n",
    "    #rel_movies = \n",
    "    \n",
    "    tokenized += stemmer(remove_stopwords(tokenize(bio)))\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "actors['tokenized'] = actors.apply(lambda row: tokenize_actor_item(row[\"name\"], row.bio_imdb+' '+row.bio_rottom+' '+row.birthday+' '+row.birthplace), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.imdb.com/title/tt5033998/?ref_=nm_flmg_act_1',\n",
       "  'data': \"2019  Charlie's Angels (post-production)\"},\n",
       " {'url': 'https://www.imdb.com/title/tt6139732/?ref_=nm_flmg_act_2',\n",
       "  'data': '2019  Aladdin (post-production)  Jasmine'},\n",
       " {'url': 'https://www.imdb.com/title/tt7224848/?ref_=nm_flmg_act_3',\n",
       "  'data': \"2017  Britain's Most Evil Killers (TV Series documentary)  Reconstruction Actor  - Dennis Nilsen (2017) ... Reconstruction Actor\"},\n",
       " {'url': 'https://www.imdb.com/title/tt3717490/?ref_=nm_flmg_act_4',\n",
       "  'data': '2017  Power Rangers  Kimberly (Pink Ranger)'},\n",
       " {'url': 'https://www.imdb.com/title/tt8311258/?ref_=nm_flmg_act_5',\n",
       "  'data': \"2017  Naomi Scott: Lover's Lies (Short)\"},\n",
       " {'url': 'https://www.imdb.com/title/tt0874608/?ref_=nm_flmg_act_6',\n",
       "  'data': '2015  Inspector Lewis (TV Series)  Sahira Desai  - One for Sorrow: Part 2 (2015) ... Sahira Desai   - One for Sorrow: Part 1 (2015) ... Sahira Desai'},\n",
       " {'url': 'https://www.imdb.com/title/tt2006295/?ref_=nm_flmg_act_7',\n",
       "  'data': '2015  The 33  Escarlette'},\n",
       " {'url': 'https://www.imdb.com/title/tt4055530/?ref_=nm_flmg_act_8',\n",
       "  'data': '2014  Hello, Again (Short)  Maura'},\n",
       " {'url': 'https://www.imdb.com/title/tt2723180/?ref_=nm_flmg_act_9',\n",
       "  'data': '2013  Our Lady of Lourdes (Short)  Lourdes'},\n",
       " {'url': 'https://www.imdb.com/title/tt2904568/?ref_=nm_flmg_act_10',\n",
       "  'data': '2013  By Any Means (TV Mini-Series)  Vanessa Velasquez  - Episode #1.3 (2013) ... Vanessa Velasquez'},\n",
       " {'url': 'https://www.imdb.com/title/tt7144044/?ref_=nm_flmg_act_11',\n",
       "  'data': '2013  Bridgit Mendler: Hurricane (Video short)'},\n",
       " {'url': 'https://www.imdb.com/title/tt3432746/?ref_=nm_flmg_act_12',\n",
       "  'data': '2012  Modern/Love (Short)  Harper'},\n",
       " {'url': 'https://www.imdb.com/title/tt8311254/?ref_=nm_flmg_act_13',\n",
       "  'data': \"2012  Adam Hicks, Hayley Kiyoko, Naomi Scott & Chris Brochu: Don't Stop the Revolution (Short)  Naomi Scott\"},\n",
       " {'url': 'https://www.imdb.com/title/tt1641349/?ref_=nm_flmg_act_14',\n",
       "  'data': '2011  Terra Nova (TV Series)  Maddy Shannon  - Resistance (2011) ... Maddy Shannon   - Occupation (2011) ... Maddy Shannon   - Within (2011) ... Maddy Shannon   - Now You See Me (2011) ... Maddy Shannon   - Vs. (2011) ... Maddy Shannon      Show all 13 episodes'},\n",
       " {'url': 'https://www.imdb.com/title/tt1648204/?ref_=nm_flmg_act_15',\n",
       "  'data': '2011  Lemonade Mouth (TV Movie)  Mohini'},\n",
       " {'url': 'https://www.imdb.com/title/tt7144126/?ref_=nm_flmg_act_16',\n",
       "  'data': '2011  Lemonade Mouth: Somebody (Short)'},\n",
       " {'url': 'https://www.imdb.com/title/tt1305832/?ref_=nm_flmg_act_17',\n",
       "  'data': '2009  Life Bites (TV Series)  Megan  - Episode #2.12 (2009) ... Megan   - Episode #2.11 (2009) ... Megan   - Episode #2.10 (2009) ... Megan   - Episode #2.9 (2009) ... Megan   - Episode #2.8 (2009) ... Megan      Show all 11 episodes'}]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_filmography(actors.head(1).filmography.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(source[:-5]+'-invertedIndexFreq.pickle', 'wb') as handle:\n",
    "    pickle.dump(invertedIndexFreq, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
