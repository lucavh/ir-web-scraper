{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter,defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Creating inverted indexes for term frequency and term positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [re.sub(r'[^\\w\\s]','',w) for w in nltk.word_tokenize(text.lower()) if re.sub(r'[^\\w\\s]','',w) != '']\n",
    "   \n",
    "def remove_stopwords(tokens):\n",
    "    en_stopwords = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in en_stopwords]\n",
    "\n",
    "def stemmer(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename\n",
    "source = 'data-hl.json'\n",
    "\n",
    "# Open json file \n",
    "with open(source) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Make dataframe from data file \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create small subset for debugging (can be commented out for final run)\n",
    "#df = df.head(50)\n",
    "\n",
    "# Tokenize content, remove stopwords and stem\n",
    "# Note : the following line includes the 'title' and 'content' column\n",
    "df['tokenized'] = df.apply(lambda row: stemmer(remove_stopwords(tokenize(row.title + ' ' + row.content))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything worked as expected\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverted index\n",
    "invertedIndexFreq = defaultdict(Counter)\n",
    "invertedIndexPos = defaultdict(Counter)\n",
    "\n",
    "corpusInfo = defaultdict(dict)\n",
    "corpusInfo['num_docs'] = df.shape[0]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    corpusInfo['doc_lengths'][row['url']] = len(row['tokenized'])\n",
    "        \n",
    "    for w in row['tokenized']:\n",
    "        invertedIndexFreq[w][row['url']]+=1\n",
    "        invertedIndexPos[w][row['url']] = [i for i, j in enumerate(row['tokenized']) if w == j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the inverted index with frequences\n",
    "with open(source[:-5]+'-invertedIndexFreq.pickle', 'wb') as handle:\n",
    "    pickle.dump(invertedIndexFreq, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the inverted index with positions\n",
    "with open(source[:-5]+'-invertedIndexPos.pickle', 'wb') as handle:\n",
    "    pickle.dump(invertedIndexFreq, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the corpus info\n",
    "with open(source[:-5]+'-corpusInfo.pickle', 'wb') as handle:\n",
    "    pickle.dump(invertedIndexFreq, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Combining all inverted indexes into one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "# Computing TF-IDF\n",
    "## !! First the seperate indexes need to be combined\n",
    "We can only compute the tf-idf if we have the word frequency in the WHOLE corpus (ALL documents), thus when all sources are combined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute tf-idf\n",
    "def tfidf(doc_freq, doc_length, n_docs_total, n_docs_containing):\n",
    "    return (doc_freq / doc_length) * math.log(n_docs_total / (1 + n_docs_containing))\n",
    "\n",
    "for w,wv in invertedIndex.items():\n",
    "    n_docs_total = corpusInfo['num_docs']\n",
    "    n_docs_containing = len(wv['freq'])\n",
    "    \n",
    "    for d_id,dv in wv['freq'].items():\n",
    "        doc_freq = dv\n",
    "        doc_length = corpusInfo['doc_lengths'][d_id]\n",
    "        \n",
    "        invertedIndex[w]['tf-idf'][d_id] = tfidf(doc_freq, doc_length, n_docs_total, n_docs_containing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
